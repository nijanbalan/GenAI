{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad229a96-e4b2-4314-875c-4e4d9909fcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\") \n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c640c66-5aa7-441e-92ff-8255118e4af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The Shawshank Redemption\n"
     ]
    }
   ],
   "source": [
    "text = \"The Shawshank\"\n",
    "\n",
    "# Tokenize the input string\n",
    "input = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "# Run the model\n",
    "output = model.generate(input, max_length=5, do_sample=False)\n",
    "\n",
    "# Print the output\n",
    "print('\\n',tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21dfb471-a765-41b9-82d3-51e0abfc2e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# This is the embedding matrix of the model\n",
    "model.transformer.wte # Dimensions are: (Number of tokens in vocabulary, dimension of model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82070c87-4add-46f8-81c1-bf51eecba21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 19081,  2024,  6429,   999,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize a sample text\n",
    "text = \"Transformers are amazing!\"\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Load a model for further processing\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Pass the tokenized input to the model\n",
    "outputs = model(**encoded_input)\n",
    "\n",
    "# Print tokenized output\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73dc6891-3928-400b-8e14-0ff66a7dae22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad522d5c9979496b80c0513d338cc156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34bc3c7332b4260ba21c8d7f41973ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028036c030f242788639e52d2174a7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6034a09f81a24ffd929505f36ec46087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba8b645b2424dc599aace79bb3f1734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1651eae45c154fce9c96e534ec4fc8cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c5ad90fb5cf4c1e90791dc6c4733eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the tokenizer and model (e.g., GPT-2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Ensure the model is in evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "db028faf-d2fb-4365-8ad8-f0df97d11da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the input text\n",
    "input_text = \"cat sat on the\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Get the model's output (logits)\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    outputs = model(input_ids)\n",
    "# Extract the logits for the next token prediction\n",
    "logits = outputs.logits  # Shape: [batch_size, sequence_length, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "983fd203-864b-4371-af97-22d1a467716b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-30.4127, -29.6745, -32.4687,  ..., -38.5662, -37.2364, -30.6165],\n",
       "         [-81.5814, -81.6505, -85.2890,  ..., -90.8278, -84.9837, -84.4120],\n",
       "         [-68.0099, -67.1430, -73.7267,  ..., -74.9736, -72.3875, -69.6694],\n",
       "         [-91.5244, -88.8403, -92.6906,  ..., -92.1368, -92.7049, -90.3369]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff02dafa-002f-4978-abb7-728b220e62e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:  floor, Probability: 0.0845\n",
      "Token:  ground, Probability: 0.0546\n",
      "Token:  bench, Probability: 0.0509\n",
      "Token:  couch, Probability: 0.0488\n",
      "Token:  edge, Probability: 0.0422\n"
     ]
    }
   ],
   "source": [
    "# Get the logits for the last token in the sequence (next token prediction)\n",
    "next_token_logits = logits[0, -1, :]  # Shape: [vocab_size]\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "# Print the probabilities of the top 5 tokens\n",
    "top_k = 5\n",
    "top_k_probabilities, top_k_indices = torch.topk(probabilities, top_k)\n",
    "\n",
    "for i in range(top_k):\n",
    "    token = tokenizer.decode([top_k_indices[i].item()])\n",
    "    prob = top_k_probabilities[i].item()\n",
    "    print(f\"Token: {token}, Probability: {prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714bf5da-d038-4034-8d3a-d8fa90a2e4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8733f-322c-4a67-9c43-1d23bb6d021c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
